---
title: "L7_B_Pozza"
author: "Francesco Pozza"
date: "2/5/2020"
output:  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1 

Let weib.y be an i.i.d sample from a gamma r.v with density $$ p(y;\theta)\, = \, \frac{1}{\Gamma{\theta}} 3^\theta y^{\theta-1} e^{-3y} \qquad y,\theta>0.  $$ 
\textbf{a) Find the m.l.e of $\theta$ solving the likelihood equation and also maximizing directily the log-likelihood.}
```{r}
# Loading the data
weib.y <- c(225, 171, 198, 189, 189, 135, 162, 135, 117, 162)
# Writing the - log-likelihood
nllik <- function(param, dati= weib.y)
{
  -sum(dgamma(dati,shape=param,rate=3,log=T))

}
nllik <- Vectorize(nllik, "param")
# Getting the mle estimate maximizing the log-likelihood

mle.est1 <- optim(1, nllik,method = "Brent",  lower = 1e-6,upper = 1000,hessian = T)
cat( "Therefore the mle is for theta is",mle.est1$par)
```
In order to obtain the etimate from the likelihood equation we need to obtain the score which is equal to 
$$ - \frac{\partial }{ \partial \theta} \log \Gamma{(\theta)}+\log(3)+\log y.$$ Since $\frac{\partial }{ \partial \theta} \log \Gamma{(\theta)}$ is implemented with the function digamma() we can write 
```{r}
score <- function(param, data=weib.y)
{
  n <- length(data)
  -n*digamma(param)+n*log(3)+sum(log(data))
}

# Getting the mle estimate
mle.est2 <- uniroot(score,interval  = c(1e-6,1000))
mle.est2$root
```
\textbf{b) Plot the relative log likelihood and find the 0.90 Wald and deviance confidence intervals.} 
\newline
The relative log-likelihood is define as $\mathrm{RL}(\theta) = \log \theta-\log \hat{\theta}$ therefore its plot can be obtain writing 
```{r rllik}
plot(function(x) -(nllik(x)-nllik(mle.est1$par)), ylim = c(-8,0),
     xlim = c(460,540), xlab=expression(theta),
     ylab = "Relative log-Likelihood", main = "Fig 1: Relative log-likelihood plot")
```
The required confidence intervals can be obtain writing
```{r}
# calculate wald interval
wald <- mle.est1$par+c(-1,1)*qnorm(0.95)*sqrt(1/drop(mle.est1$hessian))
wald
# calculate log-ratio interval 
library(rootSolve)
LR <- uniroot.all(function(x) -2*(nllik(mle.est1$par)-
                  nllik(x))-qchisq(0.90, df=1) , interval = c(1,1000) )
LR
```
and we can see that they're quite similar. 
\newline
\textbf{ c) Repeat the previous points considering the parameterization $\omega=\log \theta$ }
\newline
The mle estimate for $\omega = \log \theta$ and its standard error can be obtain using as argument of the function nllik() the quantity $\theta =e^{\omega}.$ In addition, if we want to obtain the mle using the score equation, we just need to remember that $ \frac{\partial}{\partial \omega}\ell(\omega) = \frac{\partial}{\partial \theta}\ell(\theta) \frac{\partial}{\partial \omega}\theta(\omega).$ Therefore:
```{r}
# Getting the mle estimate maximizing the log-likelihood
mle.est3 <- optim(1,function(x) nllik(exp(x)),method = "Brent",
                lower = 1e-6,upper = 1000,hessian = T)
mle.est3$par

# getting mle estimating solving the score equation
mle.est4 <- uniroot(function(x) score(exp(x))*exp(x),interval  = c(1e-6,1000))
mle.est4$root
```
Relative log-likelihood plot and confidence interval are obtain in the same way of point b). Since the Log-ratio confidence interval is equivariant to re-parametrization, we just need to take the logarithm of the interval obtained for $\theta$
```{r}
plot(function(x) -(nllik(exp(x))-nllik(exp(mle.est3$par))), ylim = c(-8,0), xlim = c(6.1,6.30)
     , xlab=expression(omega), ylab = "Relative log-Likelihood",
     main = "Fig 2: Relative log-likelihood plot")
# Wlad interval
wald.omega <- mle.est3$par+c(-1,1)*qnorm(0.975)*sqrt(1/drop(mle.est3$hessian))
wald.omega
# calculate log-ratio interval. Since this quantity is equivariat under
# reparametrizations we don't need extra calcultions
LR.omega <- log(LR)
LR.omega
```
\textbf{d) Assume a prior $N(0,\sigma^2_0)$ for $\omega$ and perform a Bayesian analysis of the data, using both analytical and simulation-based solutions.}
\newline
First of all we need to fix the hyperparameter $\sigma^2_0.$ This can be done in many ways, here we use $\sigma_0=10$ in order to have a small apriori information. At this point we can obtain the log-postirior density writing
```{r log-post}
logPost <- function(omega,iper=10,...)
{
  -nllik(exp(omega))+dnorm(omega, mean = 0, sd=iper, log = TRUE)
}
```
An analytical estimate of $\omega$ is given by the posterior mode and it can be obtain maximizing the log-posterior density
```{r MAP estimete}

MAP <- optim(1,function(x) -logPost(x), method = "Brent", hessian = T, lower = -30,
             upper = 30 ) 
MAP$par
```
In addition we can simulate from the posterior using the rejection sampling algorithm with as $g()$ a normal with $\mu = 6.21$ and $\sigma=1.$
```{r}
f <- function(x){ logPost(x)}
g <- function(x) dnorm(x,mean = MAP$par, log = T)
# Getting b
b <- -nlminb(6.2,function(x) -exp(f(x)-g(x)), lower = 1, upper = 10)$objective
# Simulation
sim <- NULL
n <- 10**2 #sample size, DA CAMBIARE
while (length(sim)<n) {
  u <- runif(n)
  x <- rnorm(n,mean = MAP$par) # qua eventualmente cambiare
  sim <- c(sim,x[(u*b*dnorm(x,mean = MAP$par)<exp(f(x)))]) # anche qua vanno cambiate le densitÃ 
}
```
Using the simulation values we can obtain an HPD interval and some descriptive indices
```{r}
library(TeachingDemos)
# histogram
hist(sim, nclass = 50, freq = F, main="Fig 3: Posterior density")
# descriptive indices
mean(sim)
quantile(sim)
# HPD interval
emp.hpd(sim,conf=0.95)
```


## Exercise 2 
Consider the following data $y_{oss}=$ (1.434313, 1.122792, 1.205189, 0.08399036, 2.700203, 1.621289,
0.5877176, 1.337052, 4.893989, 2.386678, 2.631867, 0.2630924,
3.077384, 2.815827, 0.3646507) as an i.i.d. sample from a gamma r.v. with shape  $\alpha$ and scale $1/\beta.$
\newline
\textbf{a)} Find the m.l.e. and plot the relative log likelihood.

```{r}
# loading the data
yoss <- c(1.434313, 1.122792, 1.205189, 0.08399036, 2.700203, 1.621289,
0.5877176, 1.337052, 4.893989, 2.386678, 2.631867, 0.2630924,
3.077384, 2.815827, 0.3646507)

# writing the log-likelihood
nllik <- function(param, y = yoss)
{
  -sum(dgamma(x=y, shape = param[1], scale = 1/param[2], log = T))

}

# Getting and printing the mle
mle <- optim(c(1,1),nllik, hessian = T )
mle

# Plotting the relative log-likelihood
alpha <- seq(0.01,5.5, length = 100)
beta <- seq(0.01,3, length = 100)
parvalues <- expand.grid(alpha, beta)
llikvalues <- apply(parvalues, 1, nllik, y = yoss)
llikvalues <- matrix(-llikvalues, nrow = length(alpha),
                        ncol = length(beta), byrow = F)
conf.levels<-c(0, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999)

contour(alpha, beta, llikvalues + nllik(mle$par),
        levels = -qchisq(conf.levels, 2)/2, xlab = 'alpha',
        labels = as.character(conf.levels), ylab = 'beta')
title('Fig 4: Gamma relative log likelihood')
```
\textbf{b)} Repeat the previous point with the parameterization $(\alpha, \mu) = (\alpha,\alpha/\beta)$. Comment on the results.
\newline
The maximum likelihood estimator is is invariant to re-parametrization, therefore the mle in the new parametrization is
```{r}
mle.2 <- c(mle$par[1], mle$par[1]/mle$par[2])
mle.2
```
In order to plot the relative log-likelihood we just need to get the expression of $\beta$ given $\alpha$ and $\beta$ which is $$ \beta \, = \, \frac{\alpha}{\mu}. $$
```{r}
alpha <- seq(0.01,5.5, length = 100)
mu <- seq(0.01,8, length = 100)
parvalues <- expand.grid(alpha, mu)
llikvalues <- apply(parvalues, 1, function(param) 
            nllik(c(param[1],param[1]/param[2]), y = yoss ))
llikvalues <- matrix(-llikvalues, nrow = length(alpha),
                     ncol = length(mu), byrow = F)
conf.levels<-c(0, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999)

contour(alpha, mu, llikvalues + nllik(mle$par),
        levels = -qchisq(conf.levels, 2) / 2, xlab = 'alpha',
        labels = as.character(conf.levels), ylab = 'mu')
title('Fig 5:Gamma relative log likelihood 2nd parametrization')
```
\textbf{Comments:} Comparing the relative log-likelihood in Figure 4 and Figure 5 we can see that the first has a more regular shape.

## Exercise 3 
Consider Exercise 7 pag. 158 of Davison (2003). Consider the model described in the exercise, with $Y_i$, the ith
claim, distributed as an exponential with mean $\mu_i$ and with log $\mu_i = \beta_0+\beta_1 x_i$, where $x_i$ is equal to 0 for claims of hospital A and 1 for claims of hospital B.
\newline
\textbf{a)} Write the log likelihood for $(\beta_0,\beta_1)$ and make a contour plot.
```{r}
# loading the data
y.oss <- c( 59,172,4762,1000,2885,1905,7094,6259,1950,1208,882,22793,30002,55,32591,853,
         2153,738,311,
        36539,3556,1194,1010,5000,1370,1494,55945,19772,31992,1640,1985,2977,
        1304,1176,1385)
x.oss <- as.matrix(cbind(rep(1,19+16), c( rep(0,19),rep(1,16)  )))

# writing the log-likelihood 
logL <- function(param,y=y.oss,x=x.oss )
{
  eta <- param[1]*x[,1]+param[2]*x[,2]
  lambda <- exp(eta)
  sum(log(lambda)-lambda*y)
}

# Countour plot
beta0 <- seq(-10,-7.5, length = 100)
beta1 <- seq( -2.5, 1.3, length = 100)
parvalues <- expand.grid(beta0, beta1)
llikvalues <- apply(parvalues, 1, logL)
llikvalues <- matrix(llikvalues, nrow = length(beta0),
                       ncol = length(beta1), byrow = F)
conf.levels<-c(0, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999)
contour(beta0, beta1, llikvalues - max(llikvalues),
          levels = -qchisq(conf.levels, 2) / 2, xlab = "beta0",
          labels = as.character(conf.levels), ylab = "beta1")
title('Fig 6: Exponential relative log likelihood')
```
\textbf{b)} Obtain numerically the maximum likelihood estimate and estimates of the standard errors for each component.
```{r}
# getting mle 
estimate <- nlminb(c(1,1), function(x) -logL(x))
# plotting  mle
estimate$par
# std-error
covar.matrix <- solve( optimHess(estimate$par, function(x) -logL(x) )  )
estimate.std <- sqrt(diag(covar.matrix))
estimate.std
```
\textbf{c)} Write a function that computes the profile log likelihood for $\beta_1$ and plot it in a sensible range of parameter.
\newline
```{r}
# writing the profile log-likelihood
plogL <- function(beta1,y=y.oss,x=x.oss)
{
  # getting the constrained estimate for beta0
  beta0.v <- nlminb(0,function(x) -logL(c(x,beta1)), lower = -20, upper = 20 )$par
  # profile log-lik
  logL(c(beta0.v,beta1))
}
plogL <- Vectorize(plogL,"beta1")

# plotting log-likelihood
plot(function(x) plogL(x), xlim = c(-2.3,1), ylim=c(plogL(estimate$par[2])-8,
                                                    plogL(estimate$par[2])))
title("Fig 7: Profile log-likelihood")
```
\textbf{d)} Compute a 0.95 deviance confidence interval for $\beta_1$ and compare it with the corresponding Wald interval.
\newline
The interval can be obtained writing
```{r}
library(rootSolve)
CI.deviance <- uniroot.all(function(x) 2*(plogL(estimate$par[2])-plogL(x)) - qchisq(0.95, df = 1),
                           interval = c(-4,4))
CI.deviance
```
For the wald interval we can use, instead, the results in point b)
```{r}
CI.wald <- estimate$par[2]+c(-1,1)*qnorm(0.975)*estimate.std[2]
CI.wald
```
\textbf{Comment:} The intervals are similar because the profile log-likelihood is almost quadratic, as we saw in Figure 7.
\newline
\textbf{e)} Use the profile log-likelihood for $\beta_1$ to provide a p-value for the null hypothesis that the two hospitals have the same expected claim value.
\newline
Since the null hypothesis can be written as $\beta_1=0$ we can use the fact that the quantity $2(\ell(\hat{\beta_1})-\ell(0) )$ is approximately distributed as a $\chi^2_1.$ Therefore we can compute the p-value under the null hypothesis writing
```{r}
1- pchisq(2*(plogL(estimate$par[2])-plogL(0)),1)
```
\textbf{f)} Perform a Bayesian analysis for $(\beta_0,\beta_1)$ assuming for the parameters independent normal priors with mean 0 and standard deviation 10.
\newline
First of all we write the kernel of the log-posterior
```{r}
f <- function(beta)
{  logpost <- logL(beta)+dnorm(beta[1], sd = 10, log = T)+
    dnorm(beta[2], sd = 10, log = T)
  logpost
}
```
At this point we can simulate from the posterior. Since that there are many ways to do it, here we use rejection sampling from the normal approximation of the posterior. 
```{r}
R <- 10^4
# writing the code for the simulation
post.sim <- function(R, ...)
  { #Rejection sampling from normal approximation for the posterior
     require(mvtnorm)
     k <- 2
     postmode <- optim(c(1, 1), function(x) -f(x),
                         hessian = TRUE, method = "L-BFGS-B")
     Sigma <- solve(postmode$hessian)
     thetatilde <- postmode$par
     btilde <- -optim(thetatilde, function(x)
                -exp(f(x) -dmvnorm(x, mean = thetatilde, sigma = k * Sigma, log = TRUE)),
                method = "L-BFGS-B")$value
     th <- matrix(NA, nrow = R, ncol = 2)
     p <- 0
     for (i in 1:R)
     {
       cond <- FALSE
       while ((!cond))
           {
             p <- p + 1
             ths <- as.vector(rmvnorm(1, mean = thetatilde, sigma = k * Sigma))
             u <- runif(1, 0, btilde * dmvnorm(ths, mean = thetatilde, sigma = k * Sigma))
             cond <- (exp(f(ths)) >= u)
             }
         th[i,] <- ths
         }
     list(th = th, p = R / sum(p))
}

sim <- post.sim(R)
```
Now we can plot both the simulated joint distribution and the marginal ones
```{r}
# joint posterior
beta0 <- seq(-10.5, -7.5, length = 100)
beta1 <- seq(-2.5, 1.2, length = 100)
parvalues<-expand.grid(beta0, beta1)
postvalues<-apply(parvalues, 1, f)
postvalues<-matrix(postvalues, nrow = length(beta0),
                   ncol = length(beta1), byrow = F)
smoothScatter(sim$th, xlab = "beta0", ylab = "beta1",
              main = "Fig 8: Joint posterior")
contour(beta0, beta1, exp(postvalues - max(postvalues)),
        levels = seq(0, 1, length = 20),
        drawlabels = FALSE, add = TRUE, col = 2)
# Margial posterior
hist(sim$th[,1], nclass = 50, main = "Fig 9: Marginal posterior of beta0")
hist(sim$th[,2], nclass = 50, main = "Fig 10: Marginal posterior of beta1")
```
Finally we can finish our analysis giving some summaries of the posterior 
```{r}
# posterior mean
post.mean <- apply(sim$th, 2, mean)
names(post.mean) <-c("beta0", "beta1")
post.mean
# posterior sd
post.sd <- apply(sim$th, 2, sd)
names(post.sd) <-c("beta0", "beta1")
post.sd

# posterior quantile
post.quantile <- apply(sim$th, 2, function(x) quantile(x, c(0.025, 0.975)))
colnames(post.quantile) <- c("beta0", "beta1")
post.quantile
# HPD interval
require(TeachingDemos)
HPD <- apply(sim$th, 2, emp.hpd)
colnames(HPD) <- c("beta0", "beta1")
HPD
```
\textbf{Comment:} Even in the bayesian frame-work the results are similar to the frequentist ones. For example both the mathod give confidence interval (or credible set) for $\beta_1$ including 0.